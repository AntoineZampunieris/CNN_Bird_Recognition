{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "PyTorch CUDA version: 12.6\n",
      "GPU: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check the CUDA version used by PyTorch\n",
    "print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "\n",
    "# Check the installed GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def load_images_from_directory(directory, target_size=(256, 256)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                img = image.load_img(img_path, target_size=target_size)\n",
    "                img_array = image.img_to_array(img)\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Load all images and labels\n",
    "all_images, all_labels = load_images_from_directory(r\"C:\\Users\\antoi\\Documents\\Nell_Antoine_Project\\DATA\")\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2272, 256, 256, 3)\n",
      "(2272,)\n",
      "(569, 256, 256, 3)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "validation_images = np.array(validation_images)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(validation_images.shape)\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  ...\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  ...\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  ...\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 95.  84.  92.]\n",
      "  [ 96.  85.  91.]\n",
      "  [120. 109. 113.]\n",
      "  ...\n",
      "  [ 99. 131.  82.]\n",
      "  [109. 136.  93.]\n",
      "  [109. 136.  95.]]\n",
      "\n",
      " [[149. 135. 134.]\n",
      "  [150. 136. 136.]\n",
      "  [150. 136. 135.]\n",
      "  ...\n",
      "  [ 96. 129.  76.]\n",
      "  [104. 131.  86.]\n",
      "  [105. 132.  89.]]\n",
      "\n",
      " [[175. 163. 147.]\n",
      "  [174. 162. 150.]\n",
      "  [173. 160. 154.]\n",
      "  ...\n",
      "  [ 91. 124.  71.]\n",
      "  [ 98. 126.  78.]\n",
      "  [104. 131.  86.]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Collared_Dove' 'Wren' 'Starling' 'Collared_Dove' 'Long_Tailed_Tit']\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('Bluetit'), np.str_('Jackdaw'), np.str_('Coal_Tit'), np.str_('Great_Tit'), np.str_('Carrion_Crow'), np.str_('Magpie'), np.str_('Robin'), np.str_('Wren'), np.str_('Goldfinch'), np.str_('Blackbird'), np.str_('House_Sparrow'), np.str_('Collared_Dove'), np.str_('Greenfinch'), np.str_('Dunnock'), np.str_('Song_Thrush'), np.str_('Chaffinch'), np.str_('Feral_Pigeon'), np.str_('Wood_Pigeon'), np.str_('Starling'), np.str_('Long_Tailed_Tit')}\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(set(train_labels))\n",
    "print(len(set(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimplifiedCNNModel(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=57600, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimplifiedCNNModel(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=57600, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimplifiedCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Use a dummy input to calculate the size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 256, 256)  # Batch size 1\n",
    "            x = self.pool1(F.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool2(F.relu(self.conv2(x)))\n",
    "            x = self.pool3(F.relu(self.conv3(x)))\n",
    "            self.flatten_size = x.numel()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 20)  # Assuming 20 classes\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No softmax needed\n",
    "        return x\n",
    "\n",
    "# Example of model instantiation\n",
    "model = SimplifiedCNNModel()\n",
    "print(model)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 254, 254]             448\n",
      "         MaxPool2d-2         [-1, 16, 127, 127]               0\n",
      "            Conv2d-3         [-1, 32, 125, 125]           4,640\n",
      "         MaxPool2d-4           [-1, 32, 62, 62]               0\n",
      "            Conv2d-5           [-1, 64, 60, 60]          18,496\n",
      "         MaxPool2d-6           [-1, 64, 30, 30]               0\n",
      "           Flatten-7                [-1, 57600]               0\n",
      "            Linear-8                  [-1, 256]      14,745,856\n",
      "            Linear-9                   [-1, 20]           5,140\n",
      "================================================================\n",
      "Total params: 14,774,580\n",
      "Trainable params: 14,774,580\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 17.24\n",
      "Params size (MB): 56.36\n",
      "Estimated Total Size (MB): 74.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "validation_labels_encoded = label_encoder.transform(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [ 5 19 17  5 13]\n",
      "Encoded labels shape: (2272,)\n",
      "Validation labels shape: (569,)\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded labels:\", train_labels_encoded[:5])\n",
    "print(\"Encoded labels shape:\", train_labels_encoded.shape)\n",
    "print(\"Validation labels shape:\", validation_labels_encoded.shape)\n",
    "print(\"Number of classes:\", len(set(train_labels_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Permute if in HWC format (i.e., (N, H, W, C))\n",
    "if train_images.shape[-1] == 3:\n",
    "    train_images = train_images.transpose(0, 3, 1, 2)  # (N, H, W, C) → (N, C, H, W)\n",
    "    \n",
    "# Normalize only when converting, avoid extra copies\n",
    "train_images_tensor = torch.from_numpy(train_images).float().div(255)\n",
    "train_labels_tensor = torch.from_numpy(train_labels_encoded).long()\n",
    "\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "if validation_images.shape[-1] == 3:\n",
    "    validation_images = validation_images.transpose(0, 3, 1, 2)  # (N, H, W, C) → (N, C, H, W)\n",
    "\n",
    "validation_images_tensor = torch.from_numpy(validation_images).float().div(255)\n",
    "validation_labels_tensor = torch.from_numpy(validation_labels_encoded).long()\n",
    "\n",
    "validation_dataset = TensorDataset(validation_images_tensor, validation_labels_tensor)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 3.0473928451538086\n",
      "Step 1, Loss: 3.0458953380584717\n",
      "Step 2, Loss: 3.0434937477111816\n",
      "Step 3, Loss: 3.0402889251708984\n",
      "Step 4, Loss: 3.0363733768463135\n",
      "Step 5, Loss: 3.0318312644958496\n",
      "Step 6, Loss: 3.026740074157715\n",
      "Step 7, Loss: 3.0211706161499023\n",
      "Step 8, Loss: 3.0151875019073486\n",
      "Step 9, Loss: 3.008847951889038\n",
      "Step 10, Loss: 3.0022060871124268\n",
      "Step 11, Loss: 2.995309591293335\n",
      "Step 12, Loss: 2.9882009029388428\n",
      "Step 13, Loss: 2.980919122695923\n",
      "Step 14, Loss: 2.9735004901885986\n",
      "Step 15, Loss: 2.965975284576416\n",
      "Step 16, Loss: 2.9583725929260254\n",
      "Step 17, Loss: 2.9507174491882324\n",
      "Step 18, Loss: 2.9430322647094727\n",
      "Step 19, Loss: 2.935337543487549\n",
      "Step 20, Loss: 2.927651882171631\n",
      "Step 21, Loss: 2.9199893474578857\n",
      "Step 22, Loss: 2.9123659133911133\n",
      "Step 23, Loss: 2.9047932624816895\n",
      "Step 24, Loss: 2.897282838821411\n",
      "Step 25, Loss: 2.889843225479126\n",
      "Step 26, Loss: 2.882484197616577\n",
      "Step 27, Loss: 2.8752121925354004\n",
      "Step 28, Loss: 2.8680336475372314\n",
      "Step 29, Loss: 2.8609540462493896\n",
      "Step 30, Loss: 2.8539774417877197\n",
      "Step 31, Loss: 2.8471081256866455\n",
      "Step 32, Loss: 2.8403496742248535\n",
      "Step 33, Loss: 2.8337035179138184\n",
      "Step 34, Loss: 2.8271727561950684\n",
      "Step 35, Loss: 2.820758104324341\n",
      "Step 36, Loss: 2.8144617080688477\n",
      "Step 37, Loss: 2.8082828521728516\n",
      "Step 38, Loss: 2.8022234439849854\n",
      "Step 39, Loss: 2.7962825298309326\n",
      "Step 40, Loss: 2.7904601097106934\n",
      "Step 41, Loss: 2.7847557067871094\n",
      "Step 42, Loss: 2.7791690826416016\n",
      "Step 43, Loss: 2.7736988067626953\n",
      "Step 44, Loss: 2.768343925476074\n",
      "Step 45, Loss: 2.7631030082702637\n",
      "Step 46, Loss: 2.7579755783081055\n",
      "Step 47, Loss: 2.7529592514038086\n",
      "Step 48, Loss: 2.7480525970458984\n",
      "Step 49, Loss: 2.7432541847229004\n",
      "Step 50, Loss: 2.73856258392334\n",
      "Step 51, Loss: 2.733975410461426\n",
      "Step 52, Loss: 2.7294907569885254\n",
      "Step 53, Loss: 2.725107192993164\n",
      "Step 54, Loss: 2.72082257270813\n",
      "Step 55, Loss: 2.716634750366211\n",
      "Step 56, Loss: 2.7125422954559326\n",
      "Step 57, Loss: 2.7085423469543457\n",
      "Step 58, Loss: 2.704633951187134\n",
      "Step 59, Loss: 2.7008140087127686\n",
      "Step 60, Loss: 2.697080612182617\n",
      "Step 61, Loss: 2.6934330463409424\n",
      "Step 62, Loss: 2.6898679733276367\n",
      "Step 63, Loss: 2.6863837242126465\n",
      "Step 64, Loss: 2.682978868484497\n",
      "Step 65, Loss: 2.67965030670166\n",
      "Step 66, Loss: 2.6763970851898193\n",
      "Step 67, Loss: 2.6732177734375\n",
      "Step 68, Loss: 2.6701090335845947\n",
      "Step 69, Loss: 2.667069673538208\n",
      "Step 70, Loss: 2.664098024368286\n",
      "Step 71, Loss: 2.6611924171447754\n",
      "Step 72, Loss: 2.6583502292633057\n",
      "Step 73, Loss: 2.6555709838867188\n",
      "Step 74, Loss: 2.6528525352478027\n",
      "Step 75, Loss: 2.6501927375793457\n",
      "Step 76, Loss: 2.647590398788452\n",
      "Step 77, Loss: 2.6450438499450684\n",
      "Step 78, Loss: 2.6425514221191406\n",
      "Step 79, Loss: 2.6401119232177734\n",
      "Step 80, Loss: 2.6377241611480713\n",
      "Step 81, Loss: 2.6353847980499268\n",
      "Step 82, Loss: 2.633094549179077\n",
      "Step 83, Loss: 2.6308517456054688\n",
      "Step 84, Loss: 2.6286544799804688\n",
      "Step 85, Loss: 2.6265015602111816\n",
      "Step 86, Loss: 2.6243913173675537\n",
      "Step 87, Loss: 2.6223230361938477\n",
      "Step 88, Loss: 2.620296001434326\n",
      "Step 89, Loss: 2.6183085441589355\n",
      "Step 90, Loss: 2.616358757019043\n",
      "Step 91, Loss: 2.6144471168518066\n",
      "Step 92, Loss: 2.6125714778900146\n",
      "Step 93, Loss: 2.6107306480407715\n",
      "Step 94, Loss: 2.608924150466919\n",
      "Step 95, Loss: 2.6071510314941406\n",
      "Step 96, Loss: 2.605410575866699\n",
      "Step 97, Loss: 2.60370135307312\n",
      "Step 98, Loss: 2.602022647857666\n",
      "Step 99, Loss: 2.6003735065460205\n",
      "Step 100, Loss: 2.5987534523010254\n",
      "Step 101, Loss: 2.597161054611206\n",
      "Step 102, Loss: 2.5955965518951416\n",
      "Step 103, Loss: 2.59405779838562\n",
      "Step 104, Loss: 2.5925452709198\n",
      "Step 105, Loss: 2.5910580158233643\n",
      "Step 106, Loss: 2.589594841003418\n",
      "Step 107, Loss: 2.588155508041382\n",
      "Step 108, Loss: 2.5867390632629395\n",
      "Step 109, Loss: 2.58534574508667\n",
      "Step 110, Loss: 2.5839736461639404\n",
      "Step 111, Loss: 2.58262300491333\n",
      "Step 112, Loss: 2.5812931060791016\n",
      "Step 113, Loss: 2.579983711242676\n",
      "Step 114, Loss: 2.578693389892578\n",
      "Step 115, Loss: 2.577423095703125\n",
      "Step 116, Loss: 2.5761709213256836\n",
      "Step 117, Loss: 2.574936866760254\n",
      "Step 118, Loss: 2.573721408843994\n",
      "Step 119, Loss: 2.5725221633911133\n",
      "Step 120, Loss: 2.571340560913086\n",
      "Step 121, Loss: 2.5701754093170166\n",
      "Step 122, Loss: 2.569025993347168\n",
      "Step 123, Loss: 2.567892074584961\n",
      "Step 124, Loss: 2.5667741298675537\n",
      "Step 125, Loss: 2.5656707286834717\n",
      "Step 126, Loss: 2.564582109451294\n",
      "Step 127, Loss: 2.5635080337524414\n",
      "Step 128, Loss: 2.5624470710754395\n",
      "Step 129, Loss: 2.5614001750946045\n",
      "Step 130, Loss: 2.5603668689727783\n",
      "Step 131, Loss: 2.559346914291382\n",
      "Step 132, Loss: 2.5583388805389404\n",
      "Step 133, Loss: 2.557343006134033\n",
      "Step 134, Loss: 2.5563602447509766\n",
      "Step 135, Loss: 2.555388927459717\n",
      "Step 136, Loss: 2.554429292678833\n",
      "Step 137, Loss: 2.553480863571167\n",
      "Step 138, Loss: 2.552543878555298\n",
      "Step 139, Loss: 2.5516178607940674\n",
      "Step 140, Loss: 2.5507028102874756\n",
      "Step 141, Loss: 2.549798011779785\n",
      "Step 142, Loss: 2.548902988433838\n",
      "Step 143, Loss: 2.5480189323425293\n",
      "Step 144, Loss: 2.5471439361572266\n",
      "Step 145, Loss: 2.5462796688079834\n",
      "Step 146, Loss: 2.545423746109009\n",
      "Step 147, Loss: 2.5445775985717773\n",
      "Step 148, Loss: 2.543740749359131\n",
      "Step 149, Loss: 2.5429129600524902\n",
      "Step 150, Loss: 2.542093515396118\n",
      "Step 151, Loss: 2.541283130645752\n",
      "Step 152, Loss: 2.540480852127075\n",
      "Step 153, Loss: 2.539687395095825\n",
      "Step 154, Loss: 2.5389013290405273\n",
      "Step 155, Loss: 2.5381245613098145\n",
      "Step 156, Loss: 2.5373544692993164\n",
      "Step 157, Loss: 2.536592721939087\n",
      "Step 158, Loss: 2.5358386039733887\n",
      "Step 159, Loss: 2.5350921154022217\n",
      "Step 160, Loss: 2.5343527793884277\n",
      "Step 161, Loss: 2.533620595932007\n",
      "Step 162, Loss: 2.532896041870117\n",
      "Step 163, Loss: 2.5321779251098633\n",
      "Step 164, Loss: 2.5314671993255615\n",
      "Step 165, Loss: 2.5307626724243164\n",
      "Step 166, Loss: 2.5300652980804443\n",
      "Step 167, Loss: 2.529374599456787\n",
      "Step 168, Loss: 2.5286903381347656\n",
      "Step 169, Loss: 2.528012275695801\n",
      "Step 170, Loss: 2.5273401737213135\n",
      "Step 171, Loss: 2.526674747467041\n",
      "Step 172, Loss: 2.526015281677246\n",
      "Step 173, Loss: 2.525362014770508\n",
      "Step 174, Loss: 2.524714231491089\n",
      "Step 175, Loss: 2.5240726470947266\n",
      "Step 176, Loss: 2.5234365463256836\n",
      "Step 177, Loss: 2.522806167602539\n",
      "Step 178, Loss: 2.522181272506714\n",
      "Step 179, Loss: 2.5215625762939453\n",
      "Step 180, Loss: 2.5209484100341797\n",
      "Step 181, Loss: 2.5203402042388916\n",
      "Step 182, Loss: 2.5197370052337646\n",
      "Step 183, Loss: 2.519139528274536\n",
      "Step 184, Loss: 2.5185463428497314\n",
      "Step 185, Loss: 2.517958402633667\n",
      "Step 186, Loss: 2.517375946044922\n",
      "Step 187, Loss: 2.5167980194091797\n",
      "Step 188, Loss: 2.5162250995635986\n",
      "Step 189, Loss: 2.5156569480895996\n",
      "Step 190, Loss: 2.5150928497314453\n",
      "Step 191, Loss: 2.5145347118377686\n",
      "Step 192, Loss: 2.5139803886413574\n",
      "Step 193, Loss: 2.513430595397949\n",
      "Step 194, Loss: 2.5128862857818604\n",
      "Step 195, Loss: 2.512345314025879\n",
      "Step 196, Loss: 2.5118086338043213\n",
      "Step 197, Loss: 2.511277437210083\n",
      "Step 198, Loss: 2.510749340057373\n",
      "Step 199, Loss: 2.510226249694824\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {step}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 3, 256, 256])\n",
      "Input min/max: tensor(0.) tensor(1.)\n",
      "Label shape: torch.Size([16])\n",
      "Label dtype: torch.int64\n",
      "Unique labels: tensor([ 3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 16])\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "print(\"Input shape:\", inputs.shape)          # should be (16, 3, 256, 256)\n",
    "print(\"Input min/max:\", inputs.min(), inputs.max())  # should be ~0–1\n",
    "print(\"Label shape:\", labels.shape)          # should be (16,)\n",
    "print(\"Label dtype:\", labels.dtype)          # should be torch.long\n",
    "print(\"Unique labels:\", torch.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 3.1519047072235966\n",
      "Epoch 2/1000, Loss: 3.003735559087404\n",
      "Epoch 3/1000, Loss: 2.9918064214813875\n",
      "Epoch 4/1000, Loss: 2.9913947800515404\n",
      "Epoch 5/1000, Loss: 2.9913294483238544\n",
      "Epoch 6/1000, Loss: 2.992136364251795\n",
      "Epoch 7/1000, Loss: 2.9918716323207803\n",
      "Epoch 8/1000, Loss: 2.9913684499095865\n",
      "Epoch 9/1000, Loss: 2.991327532580201\n",
      "Epoch 10/1000, Loss: 2.991953309153167\n",
      "Epoch 11/1000, Loss: 2.9914673012746893\n",
      "Epoch 12/1000, Loss: 2.9914225474209855\n",
      "Epoch 13/1000, Loss: 2.991028026795723\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#with torch.no_grad(): #doesn't interfere with training\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#    print(model(torch.randn(1, 3, 100, 100).to(device)))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n",
    "    #with torch.no_grad(): #doesn't interfere with training\n",
    "    #    print(model(torch.randn(1, 3, 100, 100).to(device)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample outputs: tensor([ 0.0375,  0.0177, -0.1196, -0.0408,  0.0601,  0.0240,  0.0783, -0.4260,\n",
      "         0.0589,  0.0623,  0.0538,  0.0540, -0.1837,  0.0886, -0.3779, -0.0043,\n",
      "         0.0972, -0.0189,  0.0651,  0.1338], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample outputs:\", outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 4.57%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradients for evaluation\n",
    "    for test_X, test_y in validation_dataloader:\n",
    "        # Move data to GPU\n",
    "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        test_outputs = model(test_X)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(test_outputs, 1)  # Get the class with highest probability\n",
    "        \n",
    "        # Update total and correct predictions\n",
    "        correct += (predicted == test_y).sum().item()\n",
    "        total += test_y.size(0)\n",
    "\n",
    "# Compute final accuracy across all batches\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
