{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "PyTorch CUDA version: 12.6\n",
      "GPU: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check the CUDA version used by PyTorch\n",
    "print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "\n",
    "# Check the installed GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def load_images_from_directory(directory, target_size=(256, 256)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_name)\n",
    "                img = image.load_img(img_path, target_size=target_size)\n",
    "                img_array = image.img_to_array(img)\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Load all images and labels\n",
    "all_images, all_labels = load_images_from_directory(r\"C:\\Users\\antoi\\Documents\\Nell_Antoine_Project\\DATA\")\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(\n",
    "    all_images, all_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2272, 256, 256, 3)\n",
      "(2272,)\n",
      "(569, 256, 256, 3)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "validation_images = np.array(validation_images)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(validation_images.shape)\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  ...\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  ...\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]]\n",
      "\n",
      " [[  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  ...\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]\n",
      "  [  0.   0.   0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 95.  84.  92.]\n",
      "  [ 96.  85.  91.]\n",
      "  [120. 109. 113.]\n",
      "  ...\n",
      "  [ 99. 131.  82.]\n",
      "  [109. 136.  93.]\n",
      "  [109. 136.  95.]]\n",
      "\n",
      " [[149. 135. 134.]\n",
      "  [150. 136. 136.]\n",
      "  [150. 136. 135.]\n",
      "  ...\n",
      "  [ 96. 129.  76.]\n",
      "  [104. 131.  86.]\n",
      "  [105. 132.  89.]]\n",
      "\n",
      " [[175. 163. 147.]\n",
      "  [174. 162. 150.]\n",
      "  [173. 160. 154.]\n",
      "  ...\n",
      "  [ 91. 124.  71.]\n",
      "  [ 98. 126.  78.]\n",
      "  [104. 131.  86.]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Collared_Dove' 'Wren' 'Starling' 'Collared_Dove' 'Long_Tailed_Tit']\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{np.str_('Bluetit'), np.str_('Jackdaw'), np.str_('Coal_Tit'), np.str_('Great_Tit'), np.str_('Carrion_Crow'), np.str_('Magpie'), np.str_('Robin'), np.str_('Wren'), np.str_('Goldfinch'), np.str_('Blackbird'), np.str_('House_Sparrow'), np.str_('Collared_Dove'), np.str_('Greenfinch'), np.str_('Dunnock'), np.str_('Song_Thrush'), np.str_('Chaffinch'), np.str_('Feral_Pigeon'), np.str_('Wood_Pigeon'), np.str_('Starling'), np.str_('Long_Tailed_Tit')}\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(set(train_labels))\n",
    "print(len(set(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimplifiedCNNModel(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=50176, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimplifiedCNNModel(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=50176, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimplifiedCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3)  # New convolutional layer\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # New pooling layer\n",
    "        \n",
    "        # Use a dummy input to calculate the size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 256, 256)  # Batch size 1\n",
    "            x = self.pool1(F.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool2(F.relu(self.conv2(x)))\n",
    "            x = self.pool3(F.relu(self.conv3(x)))\n",
    "            x = self.pool4(F.relu(self.conv4(x)))  # Pass through the new layer\n",
    "            self.flatten_size = x.numel()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 256)\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(256, 20)  # Assuming 20 classes\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.pool4(F.relu(self.conv4(x)))  # Pass through the new layer\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # No softmax needed\n",
    "        return x\n",
    "\n",
    "# Example of model instantiation\n",
    "model = SimplifiedCNNModel()\n",
    "print(model)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 254, 254]             896\n",
      "         MaxPool2d-2         [-1, 32, 127, 127]               0\n",
      "            Conv2d-3         [-1, 64, 125, 125]          18,496\n",
      "         MaxPool2d-4           [-1, 64, 62, 62]               0\n",
      "            Conv2d-5          [-1, 128, 60, 60]          73,856\n",
      "         MaxPool2d-6          [-1, 128, 30, 30]               0\n",
      "            Conv2d-7          [-1, 256, 28, 28]         295,168\n",
      "         MaxPool2d-8          [-1, 256, 14, 14]               0\n",
      "           Flatten-9                [-1, 50176]               0\n",
      "           Linear-10                  [-1, 256]      12,845,312\n",
      "          Dropout-11                  [-1, 256]               0\n",
      "           Linear-12                   [-1, 20]           5,140\n",
      "================================================================\n",
      "Total params: 13,238,868\n",
      "Trainable params: 13,238,868\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 35.89\n",
      "Params size (MB): 50.50\n",
      "Estimated Total Size (MB): 87.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "validation_labels_encoded = label_encoder.transform(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [ 5 19 17  5 13]\n",
      "Encoded labels shape: (2272,)\n",
      "Validation labels shape: (569,)\n",
      "Number of classes: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded labels:\", train_labels_encoded[:5])\n",
    "print(\"Encoded labels shape:\", train_labels_encoded.shape)\n",
    "print(\"Validation labels shape:\", validation_labels_encoded.shape)\n",
    "print(\"Number of classes:\", len(set(train_labels_encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Permute if in HWC format (i.e., (N, H, W, C))\n",
    "if train_images.shape[-1] == 3:\n",
    "    train_images = train_images.transpose(0, 3, 1, 2)  # (N, H, W, C) → (N, C, H, W)\n",
    "    \n",
    "# Normalize only when converting, avoid extra copies\n",
    "train_images_tensor = torch.from_numpy(train_images).float().div(255)\n",
    "train_labels_tensor = torch.from_numpy(train_labels_encoded).long()\n",
    "\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "if validation_images.shape[-1] == 3:\n",
    "    validation_images = validation_images.transpose(0, 3, 1, 2)  # (N, H, W, C) → (N, C, H, W)\n",
    "\n",
    "validation_images_tensor = torch.from_numpy(validation_images).float().div(255)\n",
    "validation_labels_tensor = torch.from_numpy(validation_labels_encoded).long()\n",
    "\n",
    "validation_dataset = TensorDataset(validation_images_tensor, validation_labels_tensor)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training dataloader: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19)]\n",
      "Unique labels in validation dataloader: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19)]\n"
     ]
    }
   ],
   "source": [
    "# Collect unique labels from the training dataloader\n",
    "train_dataloader_labels = set()\n",
    "for _, labels in train_dataloader:\n",
    "    train_dataloader_labels.update(labels.numpy())\n",
    "\n",
    "print(\"Unique labels in training dataloader:\", sorted(train_dataloader_labels))\n",
    "\n",
    "# Collect unique labels from the validation dataloader\n",
    "validation_dataloader_labels = set()\n",
    "for _, labels in validation_dataloader:\n",
    "    validation_dataloader_labels.update(labels.numpy())\n",
    "\n",
    "print(\"Unique labels in validation dataloader:\", sorted(validation_dataloader_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 3.0078623294830322\n",
      "Step 1, Loss: 166.11094665527344\n",
      "Step 2, Loss: 33.9303092956543\n",
      "Step 3, Loss: 4.694825649261475\n",
      "Step 4, Loss: 4.175787925720215\n",
      "Step 5, Loss: 5.2840800285339355\n",
      "Step 6, Loss: 2.943967819213867\n",
      "Step 7, Loss: 2.8658511638641357\n",
      "Step 8, Loss: 2.817486524581909\n",
      "Step 9, Loss: 2.865731954574585\n",
      "Step 10, Loss: 2.53836727142334\n",
      "Step 11, Loss: 2.3290302753448486\n",
      "Step 12, Loss: 2.0352742671966553\n",
      "Step 13, Loss: 1.691937804222107\n",
      "Step 14, Loss: 1.5872498750686646\n",
      "Step 15, Loss: 1.4516807794570923\n",
      "Step 16, Loss: 1.2571907043457031\n",
      "Step 17, Loss: 0.9844353795051575\n",
      "Step 18, Loss: 0.8010205626487732\n",
      "Step 19, Loss: 0.45562735199928284\n",
      "Step 20, Loss: 0.18504495918750763\n",
      "Step 21, Loss: 0.12374165654182434\n",
      "Step 22, Loss: 0.06038903817534447\n",
      "Step 23, Loss: 0.08944380283355713\n",
      "Step 24, Loss: 0.011347546242177486\n",
      "Step 25, Loss: 0.008968347683548927\n",
      "Step 26, Loss: 0.009516299702227116\n",
      "Step 27, Loss: 0.021746402606368065\n",
      "Step 28, Loss: 0.004193761385977268\n",
      "Step 29, Loss: 0.05584470555186272\n",
      "Step 30, Loss: 0.00678471801802516\n",
      "Step 31, Loss: 0.023212945088744164\n",
      "Step 32, Loss: 0.02462761104106903\n",
      "Step 33, Loss: 0.027282170951366425\n",
      "Step 34, Loss: 0.005044423509389162\n",
      "Step 35, Loss: 0.002550509525462985\n",
      "Step 36, Loss: 0.001428461167961359\n",
      "Step 37, Loss: 0.0008394638425670564\n",
      "Step 38, Loss: 0.0005379542708396912\n",
      "Step 39, Loss: 0.00039474357618018985\n",
      "Step 40, Loss: 0.00031854110420681536\n",
      "Step 41, Loss: 0.00024740921799093485\n",
      "Step 42, Loss: 0.0002114722301485017\n",
      "Step 43, Loss: 0.00023965004947967827\n",
      "Step 44, Loss: 0.0004677197721321136\n",
      "Step 45, Loss: 0.0005723514477722347\n",
      "Step 46, Loss: 0.00022794533288106322\n",
      "Step 47, Loss: 0.0001344782067462802\n",
      "Step 48, Loss: 0.00010466130333952606\n",
      "Step 49, Loss: 9.00317681953311e-05\n",
      "Step 50, Loss: 7.733623351668939e-05\n",
      "Step 51, Loss: 6.065648267394863e-05\n",
      "Step 52, Loss: 4.464997255126946e-05\n",
      "Step 53, Loss: 3.346081211930141e-05\n",
      "Step 54, Loss: 2.655802200024482e-05\n",
      "Step 55, Loss: 2.2209700546227396e-05\n",
      "Step 56, Loss: 1.9272851204732433e-05\n",
      "Step 57, Loss: 1.707901719782967e-05\n",
      "Step 58, Loss: 1.5419645933434367e-05\n",
      "Step 59, Loss: 1.4088036550674587e-05\n",
      "Step 60, Loss: 1.2998521015106235e-05\n",
      "Step 61, Loss: 1.202260591526283e-05\n",
      "Step 62, Loss: 1.1216160601179581e-05\n",
      "Step 63, Loss: 1.0527049198572058e-05\n",
      "Step 64, Loss: 9.904983926389832e-06\n",
      "Step 65, Loss: 9.381627933180425e-06\n",
      "Step 66, Loss: 8.880621862772387e-06\n",
      "Step 67, Loss: 8.446662832284346e-06\n",
      "Step 68, Loss: 8.049954885791522e-06\n",
      "Step 69, Loss: 7.662558346055448e-06\n",
      "Step 70, Loss: 7.317996733036125e-06\n",
      "Step 71, Loss: 7.023725174803985e-06\n",
      "Step 72, Loss: 6.74621514917817e-06\n",
      "Step 73, Loss: 6.483602646767395e-06\n",
      "Step 74, Loss: 6.239615686354227e-06\n",
      "Step 75, Loss: 6.012391168042086e-06\n",
      "Step 76, Loss: 5.794478056486696e-06\n",
      "Step 77, Loss: 5.602640158031136e-06\n",
      "Step 78, Loss: 5.418251930677798e-06\n",
      "Step 79, Loss: 5.235724984231638e-06\n",
      "Step 80, Loss: 5.077411060483428e-06\n",
      "Step 81, Loss: 4.92654680783744e-06\n",
      "Step 82, Loss: 4.7812704906391446e-06\n",
      "Step 83, Loss: 4.650893970392644e-06\n",
      "Step 84, Loss: 4.518654350249562e-06\n",
      "Step 85, Loss: 4.397589691507164e-06\n",
      "Step 86, Loss: 4.291425284463912e-06\n",
      "Step 87, Loss: 4.18712306782254e-06\n",
      "Step 88, Loss: 4.084683496330399e-06\n",
      "Step 89, Loss: 3.991556695837062e-06\n",
      "Step 90, Loss: 3.904016921296716e-06\n",
      "Step 91, Loss: 3.820201982307481e-06\n",
      "Step 92, Loss: 3.741975206139614e-06\n",
      "Step 93, Loss: 3.6674730381491827e-06\n",
      "Step 94, Loss: 3.5948335153079825e-06\n",
      "Step 95, Loss: 3.5221935377194313e-06\n",
      "Step 96, Loss: 3.4607292036525905e-06\n",
      "Step 97, Loss: 3.3936773888854077e-06\n",
      "Step 98, Loss: 3.332212600071216e-06\n",
      "Step 99, Loss: 3.2763355193310417e-06\n",
      "Step 100, Loss: 3.2204584385908674e-06\n",
      "Step 101, Loss: 3.1664442303735996e-06\n",
      "Step 102, Loss: 3.119879920632229e-06\n",
      "Step 103, Loss: 3.0751782560400898e-06\n",
      "Step 104, Loss: 3.032339009223506e-06\n",
      "Step 105, Loss: 2.987637117257691e-06\n",
      "Step 106, Loss: 2.9410728075163206e-06\n",
      "Step 107, Loss: 2.896371142924181e-06\n",
      "Step 108, Loss: 2.8609820219571702e-06\n",
      "Step 109, Loss: 2.8218678380653728e-06\n",
      "Step 110, Loss: 2.7790283638751134e-06\n",
      "Step 111, Loss: 2.741776370385196e-06\n",
      "Step 112, Loss: 2.710112539716647e-06\n",
      "Step 113, Loss: 2.6803111268236535e-06\n",
      "Step 114, Loss: 2.643059588081087e-06\n",
      "Step 115, Loss: 2.609533112263307e-06\n",
      "Step 116, Loss: 2.5834567622951e-06\n",
      "Step 117, Loss: 2.5536553494021064e-06\n",
      "Step 118, Loss: 2.5294416445831303e-06\n",
      "Step 119, Loss: 2.495914941391675e-06\n",
      "Step 120, Loss: 2.4717012365726987e-06\n",
      "Step 121, Loss: 2.4437622414552607e-06\n",
      "Step 122, Loss: 2.4195485366362846e-06\n",
      "Step 123, Loss: 2.391609768892522e-06\n",
      "Step 124, Loss: 2.367396064073546e-06\n",
      "Step 125, Loss: 2.3431823592545697e-06\n",
      "Step 126, Loss: 2.3208312995848246e-06\n",
      "Step 127, Loss: 2.2966175947658485e-06\n",
      "Step 128, Loss: 2.272403662573197e-06\n",
      "Step 129, Loss: 2.2519150206790073e-06\n",
      "Step 130, Loss: 2.2314263787848176e-06\n",
      "Step 131, Loss: 2.209075091741397e-06\n",
      "Step 132, Loss: 2.1867238046979764e-06\n",
      "Step 133, Loss: 2.1643727450282313e-06\n",
      "Step 134, Loss: 2.147609166058828e-06\n",
      "Step 135, Loss: 2.128982941940194e-06\n",
      "Step 136, Loss: 2.1103571725689108e-06\n",
      "Step 137, Loss: 2.0935935935995076e-06\n",
      "Step 138, Loss: 2.073104951705318e-06\n",
      "Step 139, Loss: 2.060066890408052e-06\n",
      "Step 140, Loss: 2.0414408936630934e-06\n",
      "Step 141, Loss: 2.0265401872165967e-06\n",
      "Step 142, Loss: 2.0116394807701e-06\n",
      "Step 143, Loss: 1.9948756744270213e-06\n",
      "Step 144, Loss: 1.976249905055738e-06\n",
      "Step 145, Loss: 1.955761035787873e-06\n",
      "Step 146, Loss: 1.9408601019677008e-06\n",
      "Step 147, Loss: 1.9296844584459905e-06\n",
      "Step 148, Loss: 1.9166463971487246e-06\n",
      "Step 149, Loss: 1.9017454633285524e-06\n",
      "Step 150, Loss: 1.8887070609707735e-06\n",
      "Step 151, Loss: 1.8738061271506012e-06\n",
      "Step 152, Loss: 1.8570426618680358e-06\n",
      "Step 153, Loss: 1.8440042595102568e-06\n",
      "Step 154, Loss: 1.8328287296753842e-06\n",
      "Step 155, Loss: 1.8253783764521359e-06\n",
      "Step 156, Loss: 1.8086149111695704e-06\n",
      "Step 157, Loss: 1.7974391539610224e-06\n",
      "Step 158, Loss: 1.786263510439312e-06\n",
      "Step 159, Loss: 1.7750878669176018e-06\n",
      "Step 160, Loss: 1.7583244016350363e-06\n",
      "Step 161, Loss: 1.7452858855904196e-06\n",
      "Step 162, Loss: 1.734110355755547e-06\n",
      "Step 163, Loss: 1.7247973573830677e-06\n",
      "Step 164, Loss: 1.7136216001745197e-06\n",
      "Step 165, Loss: 1.704308715488878e-06\n",
      "Step 166, Loss: 1.6931328445934923e-06\n",
      "Step 167, Loss: 1.685682491370244e-06\n",
      "Step 168, Loss: 1.6726442026993027e-06\n",
      "Step 169, Loss: 1.663330976953148e-06\n",
      "Step 170, Loss: 1.6521553334314376e-06\n",
      "Step 171, Loss: 1.6428423350589583e-06\n",
      "Step 172, Loss: 1.6335292229996412e-06\n",
      "Step 173, Loss: 1.6204908206418622e-06\n",
      "Step 174, Loss: 1.6111777085825452e-06\n",
      "Step 175, Loss: 1.6018647102100658e-06\n",
      "Step 176, Loss: 1.5888263078522868e-06\n",
      "Step 177, Loss: 1.5813759546290385e-06\n",
      "Step 178, Loss: 1.5702000837336527e-06\n",
      "Step 179, Loss: 1.5627496168235666e-06\n",
      "Step 180, Loss: 1.5515738596150186e-06\n",
      "Step 181, Loss: 1.5348102806456154e-06\n",
      "Step 182, Loss: 1.525497282273136e-06\n",
      "Step 183, Loss: 1.51804681536305e-06\n",
      "Step 184, Loss: 1.5105964621398016e-06\n",
      "Step 185, Loss: 1.5031459952297155e-06\n",
      "Step 186, Loss: 1.4919702380211675e-06\n",
      "Step 187, Loss: 1.4882449477227055e-06\n",
      "Step 188, Loss: 1.4826571259618504e-06\n",
      "Step 189, Loss: 1.4714813687533024e-06\n",
      "Step 190, Loss: 1.4658935469924472e-06\n",
      "Step 191, Loss: 1.4565803212462924e-06\n",
      "Step 192, Loss: 1.4472672091869754e-06\n",
      "Step 193, Loss: 1.437954210814496e-06\n",
      "Step 194, Loss: 1.4342289205160341e-06\n",
      "Step 195, Loss: 1.424915808456717e-06\n",
      "Step 196, Loss: 1.4193278730090242e-06\n",
      "Step 197, Loss: 1.413740051248169e-06\n",
      "Step 198, Loss: 1.4025644077264587e-06\n",
      "Step 199, Loss: 1.3988391174279968e-06\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {step}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 3, 256, 256])\n",
      "Input min/max: tensor(0.) tensor(1.)\n",
      "Label shape: torch.Size([64])\n",
      "Label dtype: torch.int64\n",
      "Unique labels: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 15, 16, 18, 19])\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "print(\"Input shape:\", inputs.shape)          # should be (16, 3, 256, 256)\n",
    "print(\"Input min/max:\", inputs.min(), inputs.max())  # should be ~0–1\n",
    "print(\"Label shape:\", labels.shape)          # should be (16,)\n",
    "print(\"Label dtype:\", labels.dtype)          # should be torch.long\n",
    "print(\"Unique labels:\", torch.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 3.1145753860473633\n",
      "Epoch 2/20, Loss: 2.9425357712639704\n",
      "Epoch 3/20, Loss: 2.788436942630344\n",
      "Epoch 4/20, Loss: 2.604560613632202\n",
      "Epoch 5/20, Loss: 2.3801316950056286\n",
      "Epoch 6/20, Loss: 2.208783229192098\n",
      "Epoch 7/20, Loss: 2.018860379854838\n",
      "Epoch 8/20, Loss: 1.7642997768190172\n",
      "Epoch 9/20, Loss: 1.564871655570136\n",
      "Epoch 10/20, Loss: 1.3345614671707153\n",
      "Epoch 11/20, Loss: 1.1521921687655978\n",
      "Epoch 12/20, Loss: 1.0179357396231756\n",
      "Epoch 13/20, Loss: 0.8506428334448073\n",
      "Epoch 14/20, Loss: 0.6820867392751906\n",
      "Epoch 15/20, Loss: 0.5873216291268667\n",
      "Epoch 16/20, Loss: 0.4785035451253255\n",
      "Epoch 17/20, Loss: 0.41221408049265545\n",
      "Epoch 18/20, Loss: 0.3678589132097032\n",
      "Epoch 19/20, Loss: 0.2834882272614373\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Move data to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n",
    "    #with torch.no_grad(): #doesn't interfere with training\n",
    "    #    print(model(torch.randn(1, 3, 100, 100).to(device)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample outputs: tensor([ 0.1161,  0.0493, -0.0599, -0.0096,  0.1224,  0.0800,  0.1694, -0.3416,\n",
      "         0.0904,  0.0970,  0.0821,  0.1243, -0.1120,  0.1085, -0.2959,  0.0527,\n",
      "         0.1257,  0.0366,  0.0763,  0.1200], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample outputs:\", outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.56%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradients for evaluation\n",
    "    for test_X, test_y in validation_dataloader:\n",
    "        # Move data to GPU\n",
    "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        test_outputs = model(test_X)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(test_outputs, 1)  # Get the class with highest probability\n",
    "        \n",
    "        # Update total and correct predictions\n",
    "        correct += (predicted == test_y).sum().item()\n",
    "        total += test_y.size(0)\n",
    "\n",
    "# Compute final accuracy across all batches\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
